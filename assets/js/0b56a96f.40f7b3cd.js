"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[6344],{3905:function(e,n,t){t.d(n,{Zo:function(){return u},kt:function(){return m}});var a=t(67294);function o(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function l(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach((function(n){o(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function i(e,n){if(null==e)return{};var t,a,o=function(e,n){if(null==e)return{};var t,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||(o[t]=e[t]);return o}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var s=a.createContext({}),p=function(e){var n=a.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):l(l({},n),e)),t},u=function(e){var n=p(e.components);return a.createElement(s.Provider,{value:n},e.children)},c={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},d=a.forwardRef((function(e,n){var t=e.components,o=e.mdxType,r=e.originalType,s=e.parentName,u=i(e,["components","mdxType","originalType","parentName"]),d=p(t),m=o,f=d["".concat(s,".").concat(m)]||d[m]||c[m]||r;return t?a.createElement(f,l(l({ref:n},u),{},{components:t})):a.createElement(f,l({ref:n},u))}));function m(e,n){var t=arguments,o=n&&n.mdxType;if("string"==typeof e||o){var r=t.length,l=new Array(r);l[0]=d;var i={};for(var s in n)hasOwnProperty.call(n,s)&&(i[s]=n[s]);i.originalType=e,i.mdxType="string"==typeof e?e:o,l[1]=i;for(var p=2;p<r;p++)l[p]=t[p];return a.createElement.apply(null,l)}return a.createElement.apply(null,t)}d.displayName="MDXCreateElement"},230:function(e,n,t){t.r(n),t.d(n,{assets:function(){return u},contentTitle:function(){return s},default:function(){return m},frontMatter:function(){return i},metadata:function(){return p},toc:function(){return c}});var a=t(87462),o=t(63366),r=(t(67294),t(3905)),l=["components"],i={title:"DataStream Example",sidebar_position:3},s=void 0,p={unversionedId:"modules/sort/datastream_example",id:"modules/sort/datastream_example",title:"DataStream Example",description:"Examples",source:"@site/docs/modules/sort/datastream_example.md",sourceDirName:"modules/sort",slug:"/modules/sort/datastream_example",permalink:"/docs/next/modules/sort/datastream_example",draft:!1,editUrl:"https://github.com/apache/incubator-inlong-website/edit/master/docs/modules/sort/datastream_example.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{title:"DataStream Example",sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Deployment",permalink:"/docs/next/modules/sort/quick_start"},next:{title:"Overview",permalink:"/docs/next/modules/manager/overview"}},u={},c=[{value:"Examples",id:"examples",level:2},{value:"Precondition",id:"precondition",level:2},{value:"Usage for SQL API",id:"usage-for-sql-api",level:2},{value:"MySQL to Kafka",id:"mysql-to-kafka",level:3},{value:"Kafka to Hive",id:"kafka-to-hive",level:3},{value:"Usage for Dashboard",id:"usage-for-dashboard",level:2},{value:"Usage for Manager Client Tools",id:"usage-for-manager-client-tools",level:2}],d={toc:c};function m(e){var n=e.components,t=(0,o.Z)(e,l);return(0,r.kt)("wrapper",(0,a.Z)({},d,t,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("h2",{id:"examples"},"Examples"),(0,r.kt)("p",null,"To make it easier for you to create InLong-Sort jobs, here we list some data stream configuration examples.\nThe following will introduce SQL, Dashboard, Manager Client Tools methods to use Inlong Sort."),(0,r.kt)("h2",{id:"precondition"},"Precondition"),(0,r.kt)("p",null,"Please confirm whether there is the following environment:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"JDK 1.8.x"),(0,r.kt)("li",{parentName:"ul"},"Flink 1.13.5"),(0,r.kt)("li",{parentName:"ul"},"MySQL"),(0,r.kt)("li",{parentName:"ul"},"Kafka"),(0,r.kt)("li",{parentName:"ul"},"Hadoop"),(0,r.kt)("li",{parentName:"ul"},"Hive 3.x")),(0,r.kt)("p",null,"If there is no available environment, please refer to the following steps:"),(0,r.kt)("p",null,"Step1. If no Flink cluster environment is available, you may build a Flink Standalone single cluster for use. ",(0,r.kt)("a",{parentName:"p",href:"https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/standalone/overview/"},"Flink Standalone Mode")),(0,r.kt)("p",null,"Step2. If no MySQL environment is available or binlog is not enabled, you need to install MySQL and turn on binlog. ",(0,r.kt)("a",{parentName:"p",href:"https://dev.mysql.com/doc/mysql-installation-excerpt/5.7/en/"},"MySQL Installation Guide")," and ",(0,r.kt)("a",{parentName:"p",href:"https://dev.mysql.com/doc/refman/5.7/en/replication-howto-masterbaseconfig.html"},"MySQL binlog"),"\n(You can also directly refer to Inlong MySQL Extract Node doc about MySQL turn on binlog information.)"),(0,r.kt)("p",null,"Step3. If no Kafka and Hadoop environment is available, you need to install Kafka and Hadoop. ",(0,r.kt)("a",{parentName:"p",href:"https://kafka.apache.org/quickstart"},"Kafka Installation Guide")," and ",(0,r.kt)("a",{parentName:"p",href:"https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html"},"Hadoop Installation Guide")),(0,r.kt)("p",null,"Step4. If no Hive environment is available, you need to install Hive and turn on metastore service. ",(0,r.kt)("a",{parentName:"p",href:"https://cwiki.apache.org/confluence/display/Hive//GettingStarted"},"Hive Installation Guide")),(0,r.kt)("p",null,"Step5. Download Inlong installation package ",(0,r.kt)("a",{parentName:"p",href:"https://inlong.apache.org/download/main"},"inlong-distribution-(version)-incubating-bin.tar.gz"),"\nand choose MySQL,Kafka,Hive connector dependencies in the ",(0,r.kt)("a",{parentName:"p",href:"https://inlong.apache.org/download/main"},"inlong-distribution-(version)-incubating-sort-connectors.tar.gz")),(0,r.kt)("p",null,"Step6. Put sort-dist-","[version]",".jar and MySQL,Kafka,Hive connector jars into  FLINK_HOME/lib ."),(0,r.kt)("p",null,"Note: sort-dist-","[version]",".jar in the inlong-sort  package. ",(0,r.kt)("a",{parentName:"p",href:"https://inlong.apache.org/download/main"},"inlong-distribution-(version)-incubating-bin.tar.gz")," "),(0,r.kt)("h2",{id:"usage-for-sql-api"},"Usage for SQL API"),(0,r.kt)("p",null,"This example defines the data flow for a single table. (mysql--\x3ekafka--\x3ehive)"),(0,r.kt)("h3",{id:"mysql-to-kafka"},"MySQL to Kafka"),(0,r.kt)("p",null,"Single table sync example:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"./bin/flink run -c org.apache.inlong.sort.Entrance FLINK_HOME/lib/sort-dist-[version].jar \\\n--sql.script.file /YOUR_SQL_SCRIPT_DIR/mysql-to-kafka.sql\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"mysql-to-kafka.sql")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE `table_1`(\n    PRIMARY KEY (`id`) NOT ENFORCED,\n    `id` BIGINT,\n    `name` STRING,\n    `age` INT,\n    `salary` FLOAT,\n    `ts` TIMESTAMP(2),\n    `event_type` STRING)\n    WITH (\n    'append-mode' = 'true',\n    'connector' = 'mysql-cdc-inlong',\n    'hostname' = 'localhost',\n    'username' = 'root',\n    'password' = 'password',\n    'database-name' = 'dbName',\n    'table-name' = 'tableName'\n);\n\nCREATE TABLE `table_2`(\n    `id` BIGINT,\n    `name` STRING,\n    `age` INT,\n    `salary` FLOAT,\n    `ts` TIMESTAMP(2))\n    WITH (\n    'topic' = 'topicName',-- Your kafka topic\n    'properties.bootstrap.servers' = 'localhost:9092',\n    'connector' = 'kafka',\n    'json.timestamp-format.standard' = 'SQL',\n    'json.encode.decimal-as-plain-number' = 'true',\n    'json.map-null-key.literal' = 'null',\n    'json.ignore-parse-errors' = 'true',\n    'json.map-null-key.mode' = 'DROP',\n    'format' = 'json',\n    'json.fail-on-missing-field' = 'false'\n);\n\nINSERT INTO `table_2` \n    SELECT \n    `id` AS `id`,\n    `name` AS `name`,\n    `age` AS `age`,\n    CAST(NULL as FLOAT) AS `salary`,\n    `ts` AS `ts`\n    FROM `table_1`;\n\n")),(0,r.kt)("h3",{id:"kafka-to-hive"},"Kafka to Hive"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Note:"),"  First you need to create user table in Hive."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"./bin/flink run -c org.apache.inlong.sort.Entrance FLINK_HOME/lib/sort-dist-[version].jar \\\n--sql.script.file /YOUR_SQL_SCRIPT_DIR/kafka-to-hive.sql\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"kafka-to-hive.sql")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE `table_1`(\n    `id` BIGINT,\n    `name` STRING,\n    `age` INT,\n    `salary` FLOAT,\n    `ts` TIMESTAMP(2)\n    WITH (\n    'topic' = 'topicName',-- Your kafka topic\n    'properties.bootstrap.servers' = 'localhost:9092',\n    'connector' = 'kafka',\n    'scan.startup.mode' = 'earliest-offset',\n    'json.timestamp-format.standard' = 'SQL',\n    'json.encode.decimal-as-plain-number' = 'true',\n    'json.map-null-key.literal' = 'null',\n    'json.ignore-parse-errors' = 'true',\n    'json.map-null-key.mode' = 'DROP',\n    'format' = 'json',\n    'json.fail-on-missing-field' = 'false',\n    'properties.group.id' = 'groupId'-- Your group id\n);\n\nCREATE TABLE `user`(\n    `id` BIGINT,\n    `name` STRING,\n    `age` INT,\n    `salary` FLOAT,\n    `ts` TIMESTAMP(9))\n    WITH (\n    'connector' = 'hive',\n    'default-database' = 'default',\n    'hive-version' = '3.1.2',\n    'hive-conf-dir' = 'hdfs://ip:9000/.../hive-site.xml' -- Put your hive-site.xml into HDFS\n);\n\nINSERT INTO `user` \n    SELECT \n    `id` AS `id`,\n    `name` AS `name`,\n    `age` AS `age`,\n    CAST(NULL as FLOAT) AS `salary`,\n    `ts` AS `ts`\n    FROM `table_1`;\n\n")),(0,r.kt)("p",null,"Note: Of course you can also put all the SQL in one file."),(0,r.kt)("h2",{id:"usage-for-dashboard"},"Usage for Dashboard"),(0,r.kt)("p",null,"The underlying capabilities are already available and will complement the Dashboard capabilities in the future."),(0,r.kt)("h2",{id:"usage-for-manager-client-tools"},"Usage for Manager Client Tools"),(0,r.kt)("p",null,"TODO: It will be supported in the future."))}m.isMDXComponent=!0}}]);